{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "from typing import List, Set, Dict, Tuple, NewType\n",
    "from operator import itemgetter\n",
    "from nltk.corpus import stopwords\n",
    "import csv\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer():\n",
    "    def __init__(self, pattern:str):\n",
    "        \"\"\"Initialise the regular expression which will be used to tokenize our expression.\n",
    "\n",
    "        Args:\n",
    "            pattern (str): pattern to be used.\n",
    "        \"\"\"\n",
    "        self.regexp = re.compile(pattern, re.MULTILINE | re.DOTALL)\n",
    "    \n",
    "    def tokenize_text_lines(self, text_lines:List[str]) -> List[str]:\n",
    "        \"\"\"Accepts a list of strings. Tokenizes each string and creates a list of the tokens.\n",
    "\n",
    "        Args:\n",
    "            text_lines (List[str]): List of strings.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: List of tokens produced from the input strings.\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        for line in text_lines:\n",
    "            tokens += self.regexp.findall(line)\n",
    "        return tokens\n",
    "\n",
    "def construct_stopwords_set(stopwords_file_name:str) -> Set[str]:\n",
    "    \"\"\"Reads stopwords from stopwords_file_name and saves them in a set.\n",
    "\n",
    "    Args:\n",
    "        stopwords_file_name (str): Stop words file.\n",
    "\n",
    "    Returns:\n",
    "        Set[str]: [description]\n",
    "    \"\"\"\n",
    "    with open(stopwords_file_name, 'r') as f:\n",
    "        read_stopwords = f.read().splitlines()\n",
    "    stopwords_set = set(read_stopwords)\n",
    "    stopwords_set.update(stopwords.words(\"english\"))\n",
    "    return stopwords_set\n",
    "\n",
    "class SimplePreprocessor():\n",
    "    \"\"\"Class for pre-processing text. Given a list of strings, it tokenizes them, removes stop words, lowercases and stems them.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer:SimpleTokenizer, stop_words_set:Set[str], stemmer:PorterStemmer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stop_words_set = stop_words_set\n",
    "        self.stemmer = stemmer\n",
    "    \n",
    "    @staticmethod\n",
    "    def lowercase_word(word:str) -> str:\n",
    "        return str.lower(word)\n",
    "    \n",
    "    def remove_stop_words_lowercase_and_stem(self, tokens:List[str]) -> List[str]:\n",
    "        final_tokens = []\n",
    "        for token in tokens:\n",
    "            lowercase_token = SimplePreprocessor.lowercase_word(token)\n",
    "            if lowercase_token not in self.stop_words_set:\n",
    "                stemmed_token = self.stemmer.stem(lowercase_token)\n",
    "                final_tokens.append(stemmed_token)\n",
    "        return final_tokens\n",
    "    \n",
    "    def process_text_lines(self, text_lines:List[str]) -> List[str]:\n",
    "        tokens = self.tokenizer.tokenize_text_lines(text_lines)\n",
    "        tokens = self.remove_stop_words_lowercase_and_stem(tokens)\n",
    "        return tokens\n",
    "\n",
    "def pickle_object(obj:object, file_name:str):\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def unpickle_object(file_name:str) -> object:\n",
    "    with open(file_name, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: must take into account the fact that some documents may disappear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the tsv file + extract the 3 corpora.\n",
    "# Assumption: 3 corpora Quran, OT, NT.\n",
    "def read_tsv_extract_corpora(tsv_file_name:str, corpus_names_to_int:Dict[str, int]) -> Dict[int, List[str]]:\n",
    "    corpora = dict()\n",
    "    for value in corpus_names_to_int.values():\n",
    "        corpora[value] = []\n",
    "    with open(tsv_file_name, mode='r', newline='\\n') as f:\n",
    "        read_tsv = csv.reader(f, delimiter=\"\\t\")\n",
    "        for row in read_tsv:\n",
    "            corpus_name = row[0]\n",
    "            corpus_id = corpus_names_to_int[corpus_name]\n",
    "            corpora[corpus_id].append(row[1])\n",
    "    return corpora\n",
    "\n",
    "def preprocess_corpora(corpora:Dict[int, List[str]], preprocessor:SimplePreprocessor) -> Dict[int, List[List[str]]]:\n",
    "    preprocessed_corpora = dict()\n",
    "    for key in corpora.keys():\n",
    "        preprocessed_corpora[key] = []\n",
    "        for document in corpora[key]:\n",
    "            document_terms = preprocessor.process_text_lines([document])\n",
    "            preprocessed_corpora[key].append(document_terms)\n",
    "    return preprocessed_corpora\n",
    "\n",
    "# ----------------------------------CREATE INDEX AND DOCID SET----------------------------------\n",
    "Index = NewType('Index', Dict[str, Dict[int, Dict[int, int]]])\n",
    "def read_corpora_and_create_index(corpora:Dict[int, List[List[str]]]) -> Tuple[Index, Dict[int, int]]:\n",
    "    \"\"\"Reads input trec file and creates a positional inverted index from it, and it also creates a set containing all document IDs.\n",
    "\n",
    "    Args:\n",
    "        input_file_name (str): input trec file name.\n",
    "        preprocessor (SimplePreprocessor): initialized SimplePreprocessor.\n",
    "    \"\"\"\n",
    "    index = dict()\n",
    "    corpora_nr_docs = dict()\n",
    "    \n",
    "    for corpus_id in corpora.keys():\n",
    "        corpora_nr_docs[corpus_id] = 0\n",
    "        for (doc_id, doc_tokens) in enumerate(corpora[corpus_id]):\n",
    "            corpora_nr_docs[corpus_id] += 1\n",
    "            for token in doc_tokens:\n",
    "                if token in index:\n",
    "                    if corpus_id in index[token]:\n",
    "                        if doc_id in index[token][corpus_id]:\n",
    "                            index[token][corpus_id][doc_id] += 1\n",
    "                        else:\n",
    "                            index[token][corpus_id][doc_id] = 1\n",
    "                    else:\n",
    "                        index[token][corpus_id] = dict()\n",
    "                        index[token][corpus_id][doc_id] = 1\n",
    "                else:\n",
    "                    index[token] = dict()\n",
    "                    index[token][corpus_id] = dict()\n",
    "                    index[token][corpus_id][doc_id] = 1\n",
    "                    \n",
    "                    \n",
    "        print(\"Index construction for corpus \" + str(corpus_id+1) + \" finished.\")\n",
    "\n",
    "    return index, corpora_nr_docs\n",
    "\n",
    "def calculate_freq_term(index:Index, term:str) -> int:\n",
    "    if term not in index:\n",
    "        return 0\n",
    "    \n",
    "    frequency = 0\n",
    "    for corpus_id in index[term]:\n",
    "        for doc_id in index[term][corpus_id]:\n",
    "            frequency += index[term][corpus_id][doc_id]\n",
    "    return frequency\n",
    "\n",
    "\n",
    "def remove_low_freq_words_from_index(corpora_index:Index, threshold_freq:int) -> Index:\n",
    "    new_index = dict()\n",
    "\n",
    "    for term in corpora_index:\n",
    "        freq = calculate_freq_term(corpora_index, term)\n",
    "        if freq >= threshold_freq:\n",
    "            new_index[term] = corpora_index[term]\n",
    "    return new_index\n",
    "    \n",
    "\n",
    "def compute_MI_score_term_corpus(N:int, N_00:int, N_01:int, N_10:int, N_11:int) -> float:\n",
    "    N_1x = N_10 + N_11 # 0 iff no corpus contains the term (impossible)\n",
    "    N_x1 = N_01 + N_11 # 0 iff the corpus doesn't contain any documents (may be possible with a cheater corpus)\n",
    "    N_0x = N_01 + N_00 # 0 iff ALL documents contain term t (may be possible if you miss a stop word or you tokenize incorrectly -- need to check for assignment imo)\n",
    "    N_x0 = N_10 + N_00 # 0 N_10 = 0 iff no other documents (from other corpora) contain the term. N_00 = 0 iff every document (from other corpora) contain the term.\n",
    "    # N_x0 can be 0 iff we have a single corpus.\n",
    "    \n",
    "    # 0 * log(0) = 0 by convention.\n",
    "    MI_score = 0\n",
    "    if N_10 != 0:\n",
    "        MI_score += (N_10/N) * np.log2((N * N_10)/(N_1x * N_x0))\n",
    "    \n",
    "    if N_01 != 0:\n",
    "        MI_score += (N_01/N) * np.log2((N * N_01)/(N_0x * N_x1))\n",
    "    \n",
    "    if N_11 != 0:\n",
    "        MI_score += (N_11/N) * np.log2((N * N_11)/(N_1x * N_x1))\n",
    "    \n",
    "    if N_00 != 0:\n",
    "        MI_score += (N_00/N) * np.log2((N * N_00)/(N_0x * N_x0))\n",
    "        \n",
    "    return MI_score\n",
    "\n",
    "def compute_chi_score_term_corpus(N:int, N_00:int, N_01:int, N_10:int, N_11:int) -> float:\n",
    "    chi_score_numerator = (N_11 + N_10 + N_01 + N_00) * (N_11 * N_00 - N_10 * N_01) ** 2\n",
    "    # Same warning as above. Term in all documents, in no document, or one-corpus dataset.\n",
    "    chi_score_denominator = (N_11 + N_01) * (N_11 + N_10) * (N_10 + N_00) * (N_01 + N_00)\n",
    "    chi_score = chi_score_numerator/chi_score_denominator\n",
    "    \n",
    "    return chi_score\n",
    "\n",
    "def compute_MI_chi_scores(index:Index, corpora_nr_docs:Dict[int, int], corpora_ids:List[int]) -> Tuple[Dict[int, List[Tuple[str, int]]], Dict[int, List[Tuple[str, int]]]]:\n",
    "    MI_scores = dict()\n",
    "    chi_scores = dict()\n",
    "\n",
    "    for corpus_id in corpora_ids:\n",
    "        MI_scores[corpus_id] = []\n",
    "        chi_scores[corpus_id] = []\n",
    "    \n",
    "    N = 0\n",
    "    for corpus_id in corpora_nr_docs:\n",
    "        N += corpora_nr_docs[corpus_id]\n",
    "    \n",
    "    nr_docs_which_contain_term = dict()\n",
    "    for term in index:\n",
    "        N_1x = 0\n",
    "        for corpus_id in index[term]:\n",
    "            N_1x += len(index[term][corpus_id])\n",
    "        nr_docs_which_contain_term[term] = N_1x\n",
    "    \n",
    "    for term in index:\n",
    "        for corpus_id in corpora_ids:\n",
    "            N_11 = 0\n",
    "            if corpus_id not in index[term]:\n",
    "                N_01 = corpora_nr_docs[corpus_id]\n",
    "            else:\n",
    "                for _ in index[term][corpus_id]:\n",
    "                    N_11 += 1\n",
    "                N_01 = corpora_nr_docs[corpus_id] - N_11\n",
    "            N_10 = nr_docs_which_contain_term[term] - N_11\n",
    "            N_00 = N - nr_docs_which_contain_term[term] - N_01\n",
    "\n",
    "            MI_scores[corpus_id].append((term, compute_MI_score_term_corpus(N, N_00, N_01, N_10, N_11)))\n",
    "            chi_scores[corpus_id].append((term, compute_chi_score_term_corpus(N, N_00, N_01, N_10, N_11)))\n",
    "    \n",
    "    for corpus_id in MI_scores:\n",
    "        MI_scores[corpus_id] = sorted(MI_scores[corpus_id], key=itemgetter(1), reverse=True)\n",
    "        chi_scores[corpus_id] = sorted(chi_scores[corpus_id], key=itemgetter(1), reverse=True)\n",
    "    return MI_scores, chi_scores\n",
    "\n",
    "def print_top_k_terms_for_each_corpus(MI_scores, chi_scores, int_to_corpus_names, k):\n",
    "    for corpus_id in MI_scores.keys():\n",
    "        corpus_name = int_to_corpus_names[corpus_id]\n",
    "        # print('Top ' + str(k) + ' terms in ' + corpus_name + ' by MI score: ')\n",
    "        # print(MI_scores[corpus_id][:k])\n",
    "        # print('Top ' + str(k) + ' terms in ' + corpus_name + ' by Chi-squared score: ')\n",
    "        # print(chi_scores[corpus_id][:k])\n",
    "        \n",
    "        file_name = corpus_name + '_' + 'MI.csv'\n",
    "        file_content = \"term,mi\\n\"\n",
    "        for (term, MI_score) in MI_scores[corpus_id][:k]:\n",
    "            file_content += term + ',' + str(round(MI_score, 5)) + '\\n'\n",
    "        with open(file_name, 'w') as f:\n",
    "            f.write(file_content)\n",
    "        \n",
    "        file_name = corpus_name + '_' + 'chi.csv'\n",
    "        file_content = \"term,chisq\\n\"\n",
    "        for (term, chi_score) in chi_scores[corpus_id][:k]:\n",
    "            file_content += term + ',' + str(round(chi_score, 3)) + '\\n'\n",
    "        with open(file_name, 'w') as f:\n",
    "            f.write(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_topic_words_to_file(topic_words:List[Tuple[str, float]], corpus_id:int):\n",
    "    file_name = \"topic_words_corpus_\" + str(corpus_id) + \".csv\"\n",
    "    content = \"Term,Score\\n\"\n",
    "    for term, score in topic_words:\n",
    "        content += term + \",\" + str(round(score, 3)) + '\\n'\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_topics_task(corpora:Dict[int, List[List[str]]], corpora_nr_docs:Dict[int, int], num_topics=20):\n",
    "    clean_docs = []\n",
    "    for corpus_id in corpora:\n",
    "        clean_docs += corpora[corpus_id]\n",
    "    \n",
    "    dictionary = Dictionary(clean_docs)\n",
    "    dictionary.filter_extremes(no_below=15, no_above=0.6)\n",
    "    corpus = [dictionary.doc2bow(text) for text in clean_docs]\n",
    "    lda = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=25)\n",
    "    \n",
    "    corpora_topics_scores = dict()\n",
    "    for corpus_id in corpora:\n",
    "        corpora_topics_scores[corpus_id] = dict()\n",
    "        for ii in range(num_topics):\n",
    "            corpora_topics_scores[corpus_id][ii] = 0\n",
    "    \n",
    "    # Sum topic probs for each corpus.\n",
    "    for corpus_id in corpora:\n",
    "        for doc in corpora[corpus_id]:\n",
    "            doc_topics = lda.get_document_topics(dictionary.doc2bow(doc), 0)\n",
    "            for (topic_id, topic_prob) in doc_topics:\n",
    "                corpora_topics_scores[corpus_id][topic_id] += topic_prob\n",
    "    \n",
    "    # Normalise topic probs.\n",
    "    for corpus_id in corpora:\n",
    "        corpus_nr_docs = corpora_nr_docs[corpus_id]\n",
    "        for topic_id in range(num_topics):\n",
    "            corpora_topics_scores[corpus_id][topic_id] /= corpus_nr_docs\n",
    "    \n",
    "    # Select top topic for each corpus.\n",
    "    corpora_top_topic = dict()\n",
    "    for corpus_id in corpora:\n",
    "        top_topic = -1\n",
    "        top_score = 0\n",
    "        for topic_id in range(num_topics):\n",
    "            topic_score = corpora_topics_scores[corpus_id][topic_id]\n",
    "            if topic_score > top_score:\n",
    "                top_topic = topic_id\n",
    "                top_score = topic_score\n",
    "        corpora_top_topic[corpus_id] = top_topic\n",
    "    \n",
    "    for corpus_id in corpora:\n",
    "        top_topic = corpora_top_topic[corpus_id]\n",
    "        print('Top topic for corpus: ' + str(corpus_id) + \" is topic nr \" + str(top_topic))\n",
    "        print(lda.print_topic(top_topic, 10))\n",
    "        topic_words = lda.show_topic(top_topic, 10)\n",
    "        write_topic_words_to_file(topic_words, corpus_id)\n",
    "    \n",
    "    print('\\n')\n",
    "    for ii in range(num_topics):\n",
    "        print('Topic ' + str(ii) + ': ' + str(round(corpora_topics_scores[0][ii], 3)) + ' ' + \n",
    "              str(round(corpora_topics_scores[1][ii], 3)) + ' ' + \n",
    "              str(round(corpora_topics_scores[2][ii], 3)))\n",
    "    \n",
    "    print('\\n')\n",
    "    for ii in range(num_topics):\n",
    "        print('Topic ' + str(ii) + ' words: ')\n",
    "        print(lda.print_topic(ii, 10))\n",
    "    \n",
    "    return lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index construction for corpus 1 finished.\n",
      "Index construction for corpus 2 finished.\n",
      "Index construction for corpus 3 finished.\n",
      "Top topic for corpus: 0 is topic nr 15\n",
      "0.088*\"allah\" + 0.081*\"receiv\" + 0.053*\"gate\" + 0.043*\"believ\" + 0.040*\"fear\" + 0.039*\"brother\" + 0.038*\"field\" + 0.037*\"enter\" + 0.031*\"abraham\" + 0.029*\"wall\"\n",
      "Top topic for corpus: 1 is topic nr 13\n",
      "0.308*\"god\" + 0.087*\"lord\" + 0.065*\"land\" + 0.039*\"year\" + 0.038*\"hear\" + 0.025*\"egypt\" + 0.023*\"peopl\" + 0.021*\"turn\" + 0.020*\"rejoic\" + 0.017*\"gold\"\n",
      "Top topic for corpus: 2 is topic nr 0\n",
      "0.143*\"jesu\" + 0.056*\"faith\" + 0.048*\"eye\" + 0.048*\"mine\" + 0.040*\"thing\" + 0.034*\"discipl\" + 0.030*\"jew\" + 0.029*\"heart\" + 0.024*\"bread\" + 0.022*\"wise\"\n",
      "\n",
      "\n",
      "Topic 0: 0.046 0.042 0.092\n",
      "Topic 1: 0.026 0.056 0.038\n",
      "Topic 2: 0.037 0.041 0.045\n",
      "Topic 3: 0.105 0.049 0.05\n",
      "Topic 4: 0.049 0.045 0.046\n",
      "Topic 5: 0.042 0.054 0.04\n",
      "Topic 6: 0.029 0.063 0.043\n",
      "Topic 7: 0.048 0.034 0.038\n",
      "Topic 8: 0.053 0.037 0.044\n",
      "Topic 9: 0.061 0.068 0.067\n",
      "Topic 10: 0.039 0.055 0.05\n",
      "Topic 11: 0.034 0.044 0.057\n",
      "Topic 12: 0.033 0.042 0.049\n",
      "Topic 13: 0.043 0.088 0.07\n",
      "Topic 14: 0.052 0.041 0.064\n",
      "Topic 15: 0.124 0.032 0.041\n",
      "Topic 16: 0.049 0.051 0.048\n",
      "Topic 17: 0.043 0.039 0.028\n",
      "Topic 18: 0.034 0.069 0.048\n",
      "Topic 19: 0.054 0.051 0.043\n",
      "\n",
      "\n",
      "Topic 0 words: \n",
      "0.143*\"jesu\" + 0.056*\"faith\" + 0.048*\"eye\" + 0.048*\"mine\" + 0.040*\"thing\" + 0.034*\"discipl\" + 0.030*\"jew\" + 0.029*\"heart\" + 0.024*\"bread\" + 0.022*\"wise\"\n",
      "Topic 1 words: \n",
      "0.193*\"israel\" + 0.147*\"children\" + 0.085*\"heard\" + 0.053*\"lord\" + 0.051*\"head\" + 0.045*\"voic\" + 0.041*\"woman\" + 0.028*\"sacrific\" + 0.028*\"hold\" + 0.022*\"war\"\n",
      "Topic 2 words: \n",
      "0.107*\"spirit\" + 0.078*\"cast\" + 0.065*\"flesh\" + 0.059*\"mouth\" + 0.053*\"save\" + 0.050*\"beast\" + 0.041*\"wife\" + 0.040*\"wick\" + 0.037*\"eat\" + 0.037*\"fire\"\n",
      "Topic 3 words: \n",
      "0.074*\"work\" + 0.071*\"good\" + 0.068*\"evil\" + 0.059*\"lord\" + 0.043*\"bless\" + 0.037*\"nation\" + 0.034*\"stand\" + 0.029*\"babylon\" + 0.028*\"worship\" + 0.026*\"full\"\n",
      "Topic 4 words: \n",
      "0.207*\"men\" + 0.056*\"thousand\" + 0.053*\"deliv\" + 0.050*\"hundr\" + 0.037*\"prophet\" + 0.036*\"wit\" + 0.034*\"round\" + 0.032*\"bear\" + 0.028*\"number\" + 0.027*\"measur\"\n",
      "Topic 5 words: \n",
      "0.100*\"offer\" + 0.098*\"sin\" + 0.090*\"command\" + 0.075*\"lord\" + 0.069*\"place\" + 0.060*\"mose\" + 0.030*\"gave\" + 0.029*\"made\" + 0.029*\"stood\" + 0.028*\"fell\"\n",
      "Topic 6 words: \n",
      "0.210*\"king\" + 0.119*\"behold\" + 0.108*\"citi\" + 0.044*\"judgment\" + 0.044*\"die\" + 0.042*\"cri\" + 0.039*\"gather\" + 0.037*\"kingdom\" + 0.036*\"peopl\" + 0.029*\"fall\"\n",
      "Topic 7 words: \n",
      "0.086*\"world\" + 0.062*\"midst\" + 0.058*\"judg\" + 0.042*\"send\" + 0.038*\"seed\" + 0.035*\"rais\" + 0.031*\"sign\" + 0.030*\"broken\" + 0.027*\"wind\" + 0.027*\"famili\"\n",
      "Topic 8 words: \n",
      "0.058*\"death\" + 0.056*\"put\" + 0.047*\"day\" + 0.042*\"princ\" + 0.042*\"found\" + 0.041*\"grace\" + 0.036*\"feet\" + 0.031*\"wrath\" + 0.030*\"tongu\" + 0.030*\"throne\"\n",
      "Topic 9 words: \n",
      "0.131*\"lord\" + 0.118*\"hand\" + 0.100*\"word\" + 0.047*\"speak\" + 0.036*\"angel\" + 0.032*\"blood\" + 0.031*\"truth\" + 0.030*\"law\" + 0.027*\"pray\" + 0.026*\"written\"\n",
      "Topic 10 words: \n",
      "0.169*\"hous\" + 0.078*\"time\" + 0.069*\"holi\" + 0.063*\"brethren\" + 0.059*\"lord\" + 0.055*\"pass\" + 0.045*\"face\" + 0.038*\"destroy\" + 0.034*\"walk\" + 0.033*\"tree\"\n",
      "Topic 11 words: \n",
      "0.310*\"man\" + 0.040*\"peac\" + 0.038*\"stone\" + 0.038*\"brought\" + 0.034*\"great\" + 0.031*\"mighti\" + 0.030*\"chief\" + 0.027*\"fill\" + 0.023*\"lie\" + 0.023*\"build\"\n",
      "Topic 12 words: \n",
      "0.121*\"call\" + 0.102*\"servant\" + 0.058*\"answer\" + 0.050*\"end\" + 0.050*\"dwell\" + 0.047*\"depart\" + 0.047*\"lord\" + 0.042*\"spoken\" + 0.038*\"righteous\" + 0.030*\"door\"\n",
      "Topic 13 words: \n",
      "0.308*\"god\" + 0.087*\"lord\" + 0.065*\"land\" + 0.039*\"year\" + 0.038*\"hear\" + 0.025*\"egypt\" + 0.023*\"peopl\" + 0.021*\"turn\" + 0.020*\"rejoic\" + 0.017*\"gold\"\n",
      "Topic 14 words: \n",
      "0.188*\"thing\" + 0.068*\"love\" + 0.057*\"live\" + 0.052*\"soul\" + 0.049*\"dead\" + 0.045*\"life\" + 0.039*\"give\" + 0.022*\"good\" + 0.022*\"law\" + 0.021*\"understand\"\n",
      "Topic 15 words: \n",
      "0.088*\"allah\" + 0.081*\"receiv\" + 0.053*\"gate\" + 0.043*\"believ\" + 0.040*\"fear\" + 0.039*\"brother\" + 0.038*\"field\" + 0.037*\"enter\" + 0.031*\"abraham\" + 0.029*\"wall\"\n",
      "Topic 16 words: \n",
      "0.107*\"earth\" + 0.091*\"heaven\" + 0.072*\"priest\" + 0.042*\"high\" + 0.034*\"bodi\" + 0.027*\"burn\" + 0.024*\"reign\" + 0.023*\"young\" + 0.023*\"saul\" + 0.017*\"fire\"\n",
      "Topic 17 words: \n",
      "0.110*\"david\" + 0.076*\"host\" + 0.056*\"lord\" + 0.044*\"suffer\" + 0.043*\"lift\" + 0.041*\"coven\" + 0.035*\"river\" + 0.033*\"rise\" + 0.030*\"solomon\" + 0.028*\"wast\"\n",
      "Topic 18 words: \n",
      "0.253*\"son\" + 0.119*\"father\" + 0.039*\"daughter\" + 0.031*\"water\" + 0.030*\"mother\" + 0.029*\"fruit\" + 0.027*\"drink\" + 0.023*\"lay\" + 0.021*\"tribe\" + 0.019*\"possess\"\n",
      "Topic 19 words: \n",
      "0.245*\"day\" + 0.049*\"sea\" + 0.046*\"side\" + 0.039*\"light\" + 0.036*\"night\" + 0.032*\"laid\" + 0.031*\"joy\" + 0.031*\"rest\" + 0.030*\"month\" + 0.030*\"strong\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.ldamodel.LdaModel at 0x7f51250577c0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsv_file_name = 'train_and_dev.tsv'\n",
    "stopwords_file_name = \"englishST.txt\"\n",
    "index_output_file_name = \"index.txt\"\n",
    "\n",
    "stopwords_set = construct_stopwords_set(stopwords_file_name)\n",
    "tokenizer = SimpleTokenizer('[a-zA-Z]+')\n",
    "stemmer = PorterStemmer()\n",
    "preprocessor = SimplePreprocessor(tokenizer, stopwords_set, stemmer)\n",
    "\n",
    "corpus_names_to_int = {'Quran':0, 'OT':1, 'NT':2}\n",
    "int_to_corpus_names = {0:'Quran', 1:'OT', 2:'NT'}\n",
    "corpora = read_tsv_extract_corpora(tsv_file_name, corpus_names_to_int)\n",
    "\n",
    "# Apply preprocessing to the documents in the corpus.\n",
    "# Structure of \"corpora\" changes.\n",
    "corpora = preprocess_corpora(corpora, preprocessor)\n",
    "\n",
    "\n",
    "index, corpora_nr_docs = read_corpora_and_create_index(corpora)\n",
    "\n",
    "MI_scores, chi_scores = compute_MI_chi_scores(index, corpora_nr_docs, corpus_names_to_int.values())\n",
    "print_top_k_terms_for_each_corpus(MI_scores, chi_scores, int_to_corpus_names, 10)\n",
    "\n",
    "run_topics_task(corpora, corpora_nr_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_in_corpus(corpora, word):\n",
    "    for corpus_id in corpora:\n",
    "        occurrences = 0\n",
    "        for doc in corpora[corpus_id]:\n",
    "            for token in doc:\n",
    "                if word == token:\n",
    "                    occurrences += 1\n",
    "        print('Corpus ' + str(corpus_id) + \": \" + str(occurrences) + ' occurrences.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus 0: 874 occurrences.\n",
      "Corpus 1: 6529 occurrences.\n",
      "Corpus 2: 657 occurrences.\n"
     ]
    }
   ],
   "source": [
    "count_word_in_corpus(corpora, 'lord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5612, 1: 16720, 2: 5242}\n"
     ]
    }
   ],
   "source": [
    "print(corpora_nr_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
